{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19769ec8",
   "metadata": {},
   "source": [
    "# MLT Task #2\n",
    "Implemented by Gaibaliev Emil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "24695e17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\emil_\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\emil_\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\emil_\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\emil_\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from collections import Counter\n",
    "import copy\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cca5364",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "605a18e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.gutenberg.org/files/11/11-0.txt'\n",
    "response = urllib.request.urlopen(url)\n",
    "data = response.read()\n",
    "text = data.decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea18c0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "chapters = text.split('CHAPTER')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "466f6d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "chapters = chapters[13:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cdfc58a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(chapters)):\n",
    "    chapters[i] = re.sub(\"<.*?>\", ' ', chapters[i])\n",
    "    chapters[i] = re.sub(\"\\n|\\r|_\", ' ', chapters[i])\n",
    "    chapters[i] = re.sub(\" +\", ' ', chapters[i])\n",
    "    chapters[i] = chapters[i].lower()\n",
    "    chapters[i] = nltk.sent_tokenize(chapters[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eeda1a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words(\"english\")\n",
    "stop_words_alice = stop_words.copy()\n",
    "stop_words_alice.append('alice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5e627be",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d23cd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = copy.deepcopy(chapters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8582262",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(chapters)):\n",
    "    for j in range(len(chapters[i])):\n",
    "        chapters[i][j] = re.sub(r\"[^\\w\\s]\", '', chapters[i][j])\n",
    "        chapters[i][j] = TreebankWordTokenizer().tokenize(chapters[i][j])\n",
    "        for k in range(len(chapters[i][j])):\n",
    "            if chapters[i][j][k] in stop_words_alice:\n",
    "                chapters[i][j][k] = ''\n",
    "            chapters[i][j][k] = lemmatizer.lemmatize(chapters[i][j][k])\n",
    "            #chapters[i][j][k] = stemmer.stem(chapters[i][j][k])\n",
    "        chapters[i][j] = ' '.join(chapters[i][j])\n",
    "        chapters[i][j] = re.sub(' +', ' ', chapters[i][j])\n",
    "        chapters[i][j] = chapters[i][j].strip()\n",
    "    chapters[i] = list(filter(None, chapters[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "214597f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sentences)):\n",
    "    for j in range(len(sentences[i])):\n",
    "        sentences[i][j] = re.sub(r\"[^\\w\\s]\", '', sentences[i][j])\n",
    "        sentences[i][j] = TreebankWordTokenizer().tokenize(sentences[i][j])\n",
    "        for k in range(len(sentences[i][j])):\n",
    "            if sentences[i][j][k] in stop_words:\n",
    "                sentences[i][j][k] = ''\n",
    "            sentences[i][j][k] = lemmatizer.lemmatize(sentences[i][j][k])\n",
    "            #chapters[i][j][k] = stemmer.stem(chapters[i][j][k])\n",
    "        sentences[i][j] = ' '.join(sentences[i][j])\n",
    "        sentences[i][j] = re.sub(' +', ' ', sentences[i][j])\n",
    "        sentences[i][j] = sentences[i][j].strip()\n",
    "    sentences[i] = list(filter(None, sentences[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54ab5421",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(chapters)):\n",
    "    chapters[i] = ' '.join(chapters[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbc3cc3",
   "metadata": {},
   "source": [
    "## Top 10 the most important words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc194f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(chapters)\n",
    "\n",
    "# Получим массив слов\n",
    "feature_names = vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af1f7563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Глава 1: [('little', 0.17220521068853128), ('bat', 0.16988643782420274), ('door', 0.15348731183768863), ('key', 0.15007029706505384), ('eat', 0.142497851203876), ('like', 0.12628382117158962), ('think', 0.12628382117158962), ('way', 0.12628382117158962), ('either', 0.122141015317608), ('see', 0.1148034737923542)]\n",
      "Глава 2: [('mouse', 0.3039552756661129), ('pool', 0.18672948802288214), ('little', 0.18213079295020249), ('im', 0.16254319506595163), ('swam', 0.15383635519858957), ('cat', 0.15197763783305646), ('dear', 0.14876879113377037), ('said', 0.12856291267073117), ('foot', 0.12503322697380898), ('mabel', 0.12306908415887165)]\n",
      "Глава 3: [('mouse', 0.39836073809127703), ('said', 0.3637314924975534), ('dodo', 0.3166180457768027), ('prize', 0.18433498093449507), ('lory', 0.15830902288840135), ('dry', 0.13984331178645862), ('thimble', 0.12288998728966337), ('know', 0.11767783580803198), ('bird', 0.11381735374036486), ('soon', 0.09453327840977521)]\n",
      "Глава 4: [('bill', 0.21328127215206638), ('little', 0.20904459853590468), ('window', 0.2088120810938888), ('rabbit', 0.18902275606993393), ('puppy', 0.18271057095715268), ('bottle', 0.13449768062612397), ('fan', 0.13449768062612397), ('glove', 0.13449768062612397), ('one', 0.12724453823924634), ('said', 0.12724453823924634)]\n",
      "Глава 5: [('caterpillar', 0.4615617757295751), ('said', 0.42371259011144063), ('pigeon', 0.28080432430676466), ('serpent', 0.28080432430676466), ('im', 0.14264293529214422), ('egg', 0.14040216215338233), ('youth', 0.14040216215338233), ('size', 0.11153891799146129), ('father', 0.10048250649099244), ('little', 0.0896315094466509)]\n",
      "Глава 6: [('said', 0.36981070452827197), ('cat', 0.334848307317463), ('footman', 0.27115485607807727), ('baby', 0.21346502849279989), ('mad', 0.1885657304326082), ('pig', 0.17465320513047267), ('duchess', 0.1636381635555488), ('wow', 0.13557742803903863), ('like', 0.12589300579685853), ('cook', 0.11999637391165975)]\n",
      "Глава 7: [('hatter', 0.4642506374633821), ('dormouse', 0.42999786668990253), ('said', 0.380979947891587), ('hare', 0.26517328164384824), ('march', 0.26517328164384824), ('twinkle', 0.14835241270656466), ('time', 0.10977388329079625), ('tea', 0.09847740794677802), ('draw', 0.09555508148664502), ('know', 0.09040202153359692)]\n",
      "Глава 8: [('queen', 0.4566593796836842), ('said', 0.32779456439924853), ('hedgehog', 0.21892119619029551), ('king', 0.20869963516760903), ('gardener', 0.1751369569522364), ('soldier', 0.1494735206864769), ('cat', 0.1486898157469095), ('five', 0.13160844641600059), ('executioner', 0.1313527177141773), ('procession', 0.1313527177141773)]\n",
      "Глава 9: [('turtle', 0.4176726259913634), ('said', 0.4047237047710377), ('mock', 0.40220326947316476), ('gryphon', 0.27769889008900683), ('duchess', 0.20040674222316737), ('moral', 0.18351890785344366), ('queen', 0.16094192372526825), ('went', 0.0923054063512893), ('say', 0.07668752385273273), ('day', 0.07170649008924197)]\n",
      "Глава 10: [('turtle', 0.4150203770431797), ('mock', 0.37485711474867844), ('gryphon', 0.3725129029563302), ('said', 0.2765233299577671), ('lobster', 0.24705964918033543), ('dance', 0.22941253138174006), ('beautiful', 0.16065304917800505), ('soup', 0.16065304917800505), ('join', 0.1588240601873585), ('whiting', 0.1411769423887631)]\n",
      "Глава 11: [('king', 0.4046062046178392), ('hatter', 0.36408818464863163), ('said', 0.3183162571665294), ('court', 0.29440319424352235), ('dormouse', 0.2551494350110527), ('witness', 0.2285353300685015), ('queen', 0.11595746273078916), ('juror', 0.11426766503425075), ('officer', 0.11426766503425075), ('breadandbutter', 0.09813439808117412)]\n",
      "Глава 12: [('said', 0.46300447117490895), ('king', 0.39056925361943184), ('jury', 0.19778960871641904), ('queen', 0.1469847828430523), ('sister', 0.13845272610149334), ('dream', 0.13434412826773834), ('would', 0.11802074755438856), ('slate', 0.11195344022311528), ('rabbit', 0.10788968794858046), ('fit', 0.104286842998845)]\n"
     ]
    }
   ],
   "source": [
    "for i, chapter in enumerate(X):\n",
    "    # Получим значения TF-IDF для данной главы\n",
    "    tfidf_scores = zip(feature_names, chapter.toarray()[0])\n",
    "    # Отсортируем по убыванию TF-IDF\n",
    "    sorted_items = sorted(tfidf_scores, key=lambda x: x[1], reverse=True)\n",
    "    # Выберем топ-10 слов\n",
    "    top10 = sorted_items[:10]\n",
    "    print(f\"Глава {i+1}: {top10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded2ce0d",
   "metadata": {},
   "source": [
    " CHAPTER I.     Down the Rabbit-Hole              - A House of Little Bat.</br>\n",
    " CHAPTER II.    The Pool of Tears                 - Cat, Mouse, and Pool.</br>\n",
    " CHAPTER III.   A Caucus-Race and a Long Tale     - The Competition.</br>\n",
    " CHAPTER IV.    The Rabbit Sends in a Little Bill - The Rabbit with a Little Bill.</br>\n",
    " CHAPTER V.     Advice from a Caterpillar         - Advice from a Caterpillar.</br>\n",
    " CHAPTER VI.    Pig and Pepper                    - Cat and Footman.</br>\n",
    " CHAPTER VII.   A Mad Tea-Party                   - A Tea-Party.</br>\n",
    " CHAPTER VIII.  The Queen’s Croquet-Ground        - Reception at the Queen's.</br>\n",
    " CHAPTER IX.    The Mock Turtle’s Story           - The Mock Turtle's Story.</br>\n",
    " CHAPTER X.     The Lobster Quadrille             - The Party.</br>\n",
    " CHAPTER XI.    Who Stole the Tarts?              - The Court.</br>\n",
    " CHAPTER XII.   Alice’s Evidence                  - Royal Favor.</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f98ea0",
   "metadata": {},
   "source": [
    "## The top 10 the most used verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "224df344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('say', 298), ('think', 98), ('go', 93), ('get', 65), ('little', 60), ('look', 58), ('like', 58), ('know', 57), ('come', 49), ('begin', 48)]\n"
     ]
    }
   ],
   "source": [
    "def find_top_verbs_with_alice(sentences):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    verb_counts = Counter()\n",
    "\n",
    "    for chapter in sentences:\n",
    "        for sentence in chapter:\n",
    "            if \"alice\" in sentence:\n",
    "                words = nltk.word_tokenize(sentence)\n",
    "                for word in words:\n",
    "                    lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "                    if lemma != 'alice' and lemma.isalpha():\n",
    "                        verb_counts[lemma] += 1\n",
    "\n",
    "    return verb_counts.most_common(10)\n",
    "\n",
    "top_verbs = find_top_verbs_with_alice(sentences)\n",
    "print(top_verbs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
